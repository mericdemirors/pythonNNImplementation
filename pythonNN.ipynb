{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".py dsoyaları: bütün dosyalar  \n",
    "requirements.txt: versiyonlar  \n",
    "readme.txt: kodu anlatacak  \n",
    "\n",
    "### Rapor:\n",
    "= Gizli katman sayısı nasıl etkiliyor?  \n",
    "= Katmanlardaki nöron sayısı nasıl etkiliyor?  \n",
    "= Learning rate error'u ve run time'ı nasıl etkiliyor?  \n",
    "= Optimal Epoch sayısının nasıl hesaplanacağı?  \n",
    "= Her bir parametre error'u nasıl etkiliyor?  \n",
    "\n",
    "### Teslim\n",
    ". Son dosya: ogrenciNo.zip  \n",
    ". Başlık: ODEV2  \n",
    ". Mail adresi: batuhanbardak@etu.edu.tr ve m.turhan@etu.edu.tr  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python 3.10.6 64-bit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"housing.csv\", delimiter=r\"\\s+\", names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"PRICE\"], header=None)\n",
    "dataset = dataset.sample(frac=1, random_state=201401004).reset_index(drop=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "ax = sns.heatmap(\n",
    "    dataset.corr(), \n",
    "    vmin=-1, vmax=1, center=0,\n",
    "    cmap=sns.diverging_palette(20, 220, n=200),\n",
    "    square=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.hist(layout=(3,5), figsize=(15,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset.iloc[:int(len(dataset)*0.7)][dataset.columns[:-1]]\n",
    "y_train = dataset.iloc[:int(len(dataset)*0.7)][dataset.columns[-1]]\n",
    "\n",
    "X_validation = dataset.iloc[int(len(dataset)*0.7):int(len(dataset)*0.9)][dataset.columns[:-1]]\n",
    "y_validation = dataset.iloc[int(len(dataset)*0.7):int(len(dataset)*0.9)][dataset.columns[-1]]\n",
    "\n",
    "X_test = dataset.iloc[int(len(dataset)*0.9):][dataset.columns[:-1]]\n",
    "y_test = dataset.iloc[int(len(dataset)*0.9):][dataset.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dataset.columns[:-1]:\n",
    "    X_train_col_min = X_train[col].min()\n",
    "    X_train_col_max = X_train[col].max()\n",
    "\n",
    "    X_train[col] = (X_train[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "    X_validation[col] = (X_validation[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "    X_test[col] = (X_test[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "\n",
    "\n",
    "y_train_col_min = y_train.min()\n",
    "y_train_col_max = y_train.max()\n",
    "\n",
    "y_train = (y_train - y_train_col_min) / (y_train_col_max - y_train_col_min)\n",
    "y_validation = (y_validation - y_train_col_min) / (y_train_col_max - y_train_col_min)\n",
    "y_test = (y_test - y_train_col_min) / (y_train_col_max - y_train_col_min)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_data():    \n",
    "    import pandas as pd\n",
    "    dataset = pd.read_csv(\"housing.csv\", delimiter=r\"\\s+\", names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"PRICE\"], header=None)\n",
    "    dataset = dataset.sample(frac=1, random_state=201401004).reset_index(drop=True)\n",
    "\n",
    "    X_train = dataset.iloc[:int(len(dataset)*0.7)][dataset.columns[:-1]]\n",
    "    y_train = dataset.iloc[:int(len(dataset)*0.7)][dataset.columns[-1]]\n",
    "\n",
    "    X_validation = dataset.iloc[int(len(dataset)*0.7):int(len(dataset)*0.9)][dataset.columns[:-1]]\n",
    "    y_validation = dataset.iloc[int(len(dataset)*0.7):int(len(dataset)*0.9)][dataset.columns[-1]]\n",
    "\n",
    "    X_test = dataset.iloc[int(len(dataset)*0.9):][dataset.columns[:-1]]\n",
    "    y_test = dataset.iloc[int(len(dataset)*0.9):][dataset.columns[-1]]\n",
    "\n",
    "    for col in dataset.columns[:-1]:\n",
    "        X_train_col_min = X_train[col].min()\n",
    "        X_train_col_max = X_train[col].max()\n",
    "\n",
    "        X_train[col] = (X_train[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "        X_validation[col] = (X_validation[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "        X_test[col] = (X_test[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "        dataset[col] = (dataset[col] - X_train_col_min) / (X_train_col_max - X_train_col_min)\n",
    "\n",
    "\n",
    "    y_train_col_min = y_train.min()\n",
    "    y_train_col_max = y_train.max()\n",
    "\n",
    "    y_train = (y_train - y_train_col_min) / (y_train_col_max - y_train_col_min)\n",
    "    y_validation = (y_validation - y_train_col_min) / (y_train_col_max - y_train_col_min)\n",
    "    y_test = (y_test - y_train_col_min) / (y_train_col_max - y_train_col_min)\n",
    "    dataset[dataset.columns[-1]] = (dataset[dataset.columns[-1]] - y_train_col_min) / (y_train_col_max - y_train_col_min)\n",
    "\n",
    "\n",
    "    train_dataset = dataset.iloc[:int(len(dataset)*0.8)]\n",
    "    valid_dataset = dataset.iloc[int(len(dataset)*0.8):int(len(dataset)*0.9)]\n",
    "    test_dataset = dataset.iloc[int(len(dataset)*0.9):]\n",
    "\n",
    "\n",
    "    return dataset, train_dataset, valid_dataset, test_dataset, X_train, y_train, X_validation, y_validation, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, train_dataset, valid_dataset, test_dataset, X_train, y_train, X_validation, y_validation, X_test, y_test = set_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.to_csv(\"housing_for_Cpp_train.csv\", header=False, index=False)\n",
    "valid_dataset.to_csv(\"housing_for_Cpp_valid.csv\", header=False, index=False)\n",
    "test_dataset.to_csv(\"housing_for_Cpp_test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 51, 51)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset), len(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def create(self, hidden_layers=[], activation_function=\"sigmoid\", learning_rate=0.005):\n",
    "        import numpy as np\n",
    "        # Creating layer weights and biasses\n",
    "        if len(hidden_layers)!=0:\n",
    "            self.weights = []\n",
    "            self.biasses = []\n",
    "            for (layer1, layer2) in zip(hidden_layers[:-1],hidden_layers[1:]):\n",
    "                np.random.seed(int(len(hidden_layers) * learning_rate)**2)\n",
    "                self.weights.append(np.random.uniform(low=-1, high=1, size=(layer1, layer2)))\n",
    "                np.random.seed(int(len(hidden_layers) * learning_rate)**2)\n",
    "                self.biasses.append(np.random.uniform(low=-1, high=1, size=(layer2, 1)))   \n",
    "        else:\n",
    "            raise Exception(\"you must give layer sizes\")\n",
    "            \n",
    "        # declaring activation function\n",
    "        def RelU(X):\n",
    "            return np.maximum(0, X)\n",
    "        def sigmoid(X):\n",
    "            return np.exp(X)/(1 + np.exp(X))\n",
    "        def tanh(X):\n",
    "            return (np.exp(X) - np.exp(-X))/(np.exp(X) + np.exp(-X))\n",
    "        if activation_function==\"RelU\":\n",
    "            self.activation_func = RelU\n",
    "        elif activation_function==\"sigmoid\":\n",
    "            self.activation_func = sigmoid\n",
    "        elif activation_function==\"tanh\":\n",
    "            self.activation_func = tanh\n",
    "            \n",
    "        # declaring derivative of activation function\n",
    "        def derivative_RelU(X):\n",
    "            X[X<0]=0\n",
    "            X[X>0]=1\n",
    "            return X\n",
    "        def derivative_sigmoid(X):\n",
    "            return sigmoid(X)*(1-sigmoid(X))\n",
    "        def derivative_tanh(X):\n",
    "            return 1 - tanh(X)**2\n",
    "        if activation_function==\"RelU\":\n",
    "            self.derivative_activation_func = derivative_RelU\n",
    "        elif activation_function==\"sigmoid\":\n",
    "            self.derivative_activation_func = derivative_sigmoid\n",
    "        elif activation_function==\"tanh\":\n",
    "            self.derivative_activation_func = derivative_tanh\n",
    "\n",
    "        self.alpha = learning_rate\n",
    "\n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def loss(self, actual, pred):\n",
    "        import numpy as np\n",
    "        actual = np.array([a[0] for a in actual])\n",
    "        pred = np.array([p[0] for p in pred])\n",
    "        differences = actual-pred\n",
    "        differences_squared = differences*differences\n",
    "        differences_squared_sum = differences_squared.sum()\n",
    "        return differences_squared_sum/len(differences)\n",
    "        \n",
    "    def derivative_loss(self, actual, pred):\n",
    "        return (2/len(actual))*((pred-actual).sum())\n",
    "\n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def forward_prop(self, X):\n",
    "        import numpy as np\n",
    "        Zs = []\n",
    "        As = []\n",
    "        for weight, bias in zip(self.weights, self.biasses):\n",
    "            Z = weight.T @ (np.reshape(X, (len(X),1))) + bias\n",
    "            Zs.append(Z)\n",
    "            X = self.activation_func(Z)\n",
    "            As.append(X)\n",
    "        return Zs, As\n",
    "\n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def back_prop(self, weights, Zs, As, X, Y):\n",
    "        import numpy as np\n",
    "        bias_errors = []\n",
    "        weight_errors = []\n",
    "        for layer in range(len(self.weights)-1, -1, -1):\n",
    "            # calculating error\n",
    "            if layer == len(self.weights)-1: # output\n",
    "                error = self.derivative_loss(Y, As[-1]) * self.derivative_activation_func(Zs[-1])\n",
    "                bias_errors.insert(0, error)\n",
    "            else: \n",
    "                error = (weights[layer+1] @ error) * self.derivative_activation_func(Zs[layer])\n",
    "                bias_errors.insert(0, error)\n",
    "\n",
    "            # calculating weight error\n",
    "            if layer == 0: # input\n",
    "                weight_errors.insert(0, np.reshape(X, (len(X),1)) @ error.T)\n",
    "            else:\n",
    "                weight_errors.insert(0, As[layer-1] @ error.T)\n",
    "\n",
    "        return bias_errors, weight_errors  \n",
    "    \n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def update_parameters(self, alpha, bias_errors, weight_errors):\n",
    "        new_weights = self.weights.copy()\n",
    "        new_biasses = self.biasses.copy()\n",
    "        \n",
    "        for bi in range(len(new_biasses)):\n",
    "            new_biasses[bi] = self.biasses[bi] - alpha*bias_errors[bi] \n",
    "        for wi in range(len(new_weights)):\n",
    "            new_weights[wi] = self.weights[wi] - alpha*weight_errors[wi]       \n",
    "\n",
    "        self.biasses = new_biasses\n",
    "        self.weights = new_weights\n",
    "\n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def gradient_descent(self, mini_batch):\n",
    "        import warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        import numpy as np\n",
    "        total_bias_error = None\n",
    "        total_weight_error = None\n",
    "\n",
    "        for (X, Y) in mini_batch:\n",
    "            batch_Zs, batch_As = self.forward_prop(X)\n",
    "            batch_bias_error, batch_weight_error = self.back_prop(self.weights, batch_Zs, batch_As, X, Y)\n",
    "\n",
    "            if total_bias_error == None:\n",
    "                total_bias_error = np.array(batch_bias_error)\n",
    "            else:\n",
    "                total_bias_error = np.array(total_bias_error) + np.array(batch_bias_error)\n",
    "            \n",
    "            if total_weight_error == None:\n",
    "                total_weight_error = np.array(batch_weight_error)\n",
    "            else:\n",
    "                total_weight_error = np.array(total_weight_error) + np.array(batch_weight_error)\n",
    "                \n",
    "        return total_bias_error/len(mini_batch), total_weight_error/len(mini_batch)\n",
    "\n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def fit(self, epoch, batch_size, X, Y, VX, VY, verbose=1):\n",
    "        from datetime import datetime\n",
    "        import numpy as np\n",
    "        losses = []\n",
    "        current_learning_rate = self.alpha\n",
    "        for e in range(epoch):\n",
    "            if epoch != 0 and e % 100 == 0:\n",
    "                current_learning_rate = current_learning_rate * 0.25\n",
    "                \n",
    "            if verbose==1 and epoch != 0 and e % int(epoch**0.5) == 0:\n",
    "                print(datetime.now().strftime(\"%H:%M:%S\"), end=\" ----- \")\n",
    "                print(\"epoch:\",e+1, end=\" ----- \")\n",
    "\n",
    "            SEED = np.random.randint(0, 201401004)\n",
    "            np.random.seed(SEED)\n",
    "            np.random.shuffle(X)\n",
    "            np.random.seed(SEED)\n",
    "            np.random.shuffle(Y)\n",
    "            np.random.seed(SEED)\n",
    "            np.random.shuffle(VX)\n",
    "            np.random.seed(SEED)\n",
    "            np.random.shuffle(VY)\n",
    "\n",
    "            train_data = [(x_row, y) for x_row,  y in zip(X,Y)]\n",
    "            validation_data = [(x_row, y) for x_row,  y in zip(VX,VY)]\n",
    "\n",
    "            batch_number = np.floor(len(train_data)/batch_size)\n",
    "            batch_sizes = [batch_size]*int(batch_number)\n",
    "            if batch_size*batch_number != len(train_data):\n",
    "                batch_sizes.append(int(len(train_data) - batch_size*batch_number))\n",
    "            batches = np.split(train_data, np.cumsum(batch_sizes))[:-1]\n",
    "\n",
    "            for b,batch in enumerate(batches):\n",
    "                batch_bias_error, batch_weight_error = self.gradient_descent(batch)\n",
    "                self.update_parameters(current_learning_rate, batch_bias_error, batch_weight_error)\n",
    "\n",
    "            epoch_train_loss = self.loss(np.array(train_data)[:,1], self.predict(np.array(train_data)[:,0]))\n",
    "            epoch_validation_loss = self.loss(np.array(validation_data)[:,1], self.predict(np.array(validation_data)[:,0]))\n",
    "            losses.append((epoch_train_loss, epoch_validation_loss))\n",
    "            if verbose==1 and epoch != 0 and e % int(epoch**0.5) == 0:\n",
    "                print(\"train loss:\", epoch_train_loss, \"-----\" ,\"validation loss:\", epoch_validation_loss)\n",
    "\n",
    "        return losses\n",
    "        \n",
    "    ######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######---######\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            _, As = self.forward_prop(x)\n",
    "            predictions.append(As[-1][0])\n",
    "        return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gizli katman sayısının 0, 1, ve 2 olduğundaki eğitim ve test hata fonksiyonlarını çizdirerek gösteriniz.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_layer_test(layers):\n",
    "    X_train, y_train, X_validation, y_validation, X_test, y_test = set_data()\n",
    "    import numpy as np\n",
    "    X = np.reshape(np.array(X_train), (len(X_train), len(X_train.iloc[0])))\n",
    "    Y = np.reshape(np.array(y_train), (len(y_train), 1))\n",
    "    X_v = np.reshape(np.array(X_validation), (len(X_validation), len(X_validation.iloc[0])))\n",
    "    Y_v = np.reshape(np.array(y_validation), (len(y_validation), 1))\n",
    "    X_t = np.reshape(np.array(X_test), (len(X_test), len(X_test.iloc[0])))\n",
    "    Y_t = np.reshape(np.array(y_test), (len(y_test), 1))\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    nn = NN()\n",
    "    nn.create(layers, \"sigmoid\", 0.01)\n",
    "    initial_prediction = nn.predict(X_v)\n",
    "\n",
    "    losses = nn.fit(250, 1, X, Y, X_v, Y_v)\n",
    "    learned_prediction = nn.predict(X_v)\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(losses)), [v for (t,v) in losses], label = \"validation error\")\n",
    "    plt.plot(range(len(losses)), [t for (t,v) in losses], label = \"train error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    trues = [y[0] for y in Y_v]\n",
    "    trues.sort()\n",
    "    initials = [p[0] for p in initial_prediction]\n",
    "    initials.sort()\n",
    "    predictions = [p[0] for p in learned_prediction]\n",
    "    predictions.sort()\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(Y_v)), trues, label = \"true\")\n",
    "    plt.plot(range(len(Y_v)), initials, label = \"validation predictions before fit\")\n",
    "    plt.plot(range(len(Y_v)), predictions, label = \"validation predictions after fit\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    test_predictions = nn.predict(X_t)\n",
    "    import matplotlib.pyplot as plt\n",
    "    trues = [y[0] for y in Y_t]\n",
    "    trues.sort()\n",
    "    test_prediction = [p[0] for p in test_predictions]\n",
    "    test_prediction.sort()\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(Y_t)), trues, label = \"true\")\n",
    "    plt.plot(range(len(Y_t)), test_prediction, label = \"test predictions\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return nn.loss(Y_t, nn.predict(X_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gizli katman sayısı 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = hidden_layer_test([13,1])\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gizli katman sayısı 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = hidden_layer_test([13, 26, 1])\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gizli katman sayısı 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = hidden_layer_test([13, 26, 13, 1])\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate parametresinin (0.1, 0.01 vb.) değiştirilmesinin eğitim hatasını çizdirerek gösteriniz. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_test(lr):\n",
    "    X_train, y_train, X_validation, y_validation, X_test, y_test = set_data()\n",
    "    import numpy as np\n",
    "    X = np.reshape(np.array(X_train), (len(X_train), len(X_train.iloc[0])))\n",
    "    Y = np.reshape(np.array(y_train), (len(y_train), 1))\n",
    "    X_v = np.reshape(np.array(X_validation), (len(X_validation), len(X_validation.iloc[0])))\n",
    "    Y_v = np.reshape(np.array(y_validation), (len(y_validation), 1))\n",
    "    X_t = np.reshape(np.array(X_test), (len(X_test), len(X_test.iloc[0])))\n",
    "    Y_t = np.reshape(np.array(y_test), (len(y_test), 1))\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    nn = NN()\n",
    "    nn.create([13, 26, 1], \"sigmoid\", lr)\n",
    "    initial_prediction = nn.predict(X)\n",
    "\n",
    "    losses = nn.fit(250, 1, X, Y, X_v, Y_v)\n",
    "    learned_prediction = nn.predict(X)\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(losses)), [v for (t,v) in losses], label = \"validation error\")\n",
    "    plt.plot(range(len(losses)), [t for (t,v) in losses], label = \"train error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    trues = [y[0] for y in Y]\n",
    "    trues.sort()\n",
    "    initials = [p[0] for p in initial_prediction]\n",
    "    initials.sort()\n",
    "    predictions = [p[0] for p in learned_prediction]\n",
    "    predictions.sort()\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(Y)), trues, label = \"true\")\n",
    "    plt.plot(range(len(Y)), initials, label = \"train predictions before fit\")\n",
    "    plt.plot(range(len(Y)), predictions, label = \"train predictions after fit\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    test_predictions = nn.predict(X_t)\n",
    "    import matplotlib.pyplot as plt\n",
    "    trues = [y[0] for y in Y_t]\n",
    "    trues.sort()\n",
    "    test_prediction = [p[0] for p in test_predictions]\n",
    "    test_prediction.sort()\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(Y_t)), trues, label = \"true\")\n",
    "    plt.plot(range(len(Y_t)), test_prediction, label = \"test predictions\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return nn.loss(Y_t, nn.predict(X_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = learning_rate_test(0.1)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = learning_rate_test(0.01)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### learning rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = learning_rate_test(0.001)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Katmanlardaki nöron sayısının değişiminin, eğitim ve test hatalarına etkisini gözlemleyiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neuron_number_test(neuron_number):\n",
    "    X_train, y_train, X_validation, y_validation, X_test, y_test = set_data()\n",
    "    import numpy as np\n",
    "    X = np.reshape(np.array(X_train), (len(X_train), len(X_train.iloc[0])))\n",
    "    Y = np.reshape(np.array(y_train), (len(y_train), 1))\n",
    "    X_v = np.reshape(np.array(X_validation), (len(X_validation), len(X_validation.iloc[0])))\n",
    "    Y_v = np.reshape(np.array(y_validation), (len(y_validation), 1))\n",
    "    X_t = np.reshape(np.array(X_test), (len(X_test), len(X_test.iloc[0])))\n",
    "    Y_t = np.reshape(np.array(y_test), (len(y_test), 1))\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    nn = NN()\n",
    "    nn.create([13, neuron_number, 1], \"sigmoid\", 0.1)\n",
    "    initial_prediction = nn.predict(X)\n",
    "\n",
    "    losses = nn.fit(250, 1, X, Y, X_v, Y_v)\n",
    "    learned_prediction = nn.predict(X)\n",
    "\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(losses)), [v for (t,v) in losses], label = \"validation error\")\n",
    "    plt.plot(range(len(losses)), [t for (t,v) in losses], label = \"train error\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    trues = [y[0] for y in Y]\n",
    "    trues.sort()\n",
    "    initials = [p[0] for p in initial_prediction]\n",
    "    initials.sort()\n",
    "    predictions = [p[0] for p in learned_prediction]\n",
    "    predictions.sort()\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(Y)), trues, label = \"true\")\n",
    "    plt.plot(range(len(Y)), initials, label = \"train predictions before fit\")\n",
    "    plt.plot(range(len(Y)), predictions, label = \"train predictions after fit\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    test_predictions = nn.predict(X_t)\n",
    "    import matplotlib.pyplot as plt\n",
    "    trues = [y[0] for y in Y_t]\n",
    "    trues.sort()\n",
    "    test_prediction = [p[0] for p in test_predictions]\n",
    "    test_prediction.sort()\n",
    "\n",
    "    # plot lines\n",
    "    plt.plot(range(len(Y_t)), trues, label = \"true\")\n",
    "    plt.plot(range(len(Y_t)), test_prediction, label = \"test predictions\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return nn.loss(Y_t, nn.predict(X_t))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = neuron_number_test(30)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 60 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = neuron_number_test(60)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 90 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = neuron_number_test(90)\n",
    "print(\"test loss:\", test_loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch ve eğitim/validasyon hatası arasındaki ilişkiyi gösteren figür çizdiriniz.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_validation, y_validation, X_test, y_test = set_data()\n",
    "import numpy as np\n",
    "X_train = np.reshape(np.array(X_train), (len(X_train), len(X_train.iloc[0])))\n",
    "y_train = np.reshape(np.array(y_train), (len(y_train), 1))\n",
    "X_validation = np.reshape(np.array(X_validation), (len(X_validation), len(X_validation.iloc[0])))\n",
    "y_validation = np.reshape(np.array(y_validation), (len(y_validation), 1))\n",
    "X_test = np.reshape(np.array(X_test), (len(X_test), len(X_test.iloc[0])))\n",
    "y_test = np.reshape(np.array(y_test), (len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nn = NN()\n",
    "nn.create([13, 30, 1], \"sigmoid\", 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prediction = nn.predict(X_validation)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"NN mean squared error:\", mean_squared_error(y_validation, np.array(initial_prediction), squared=True))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"NN root mean squared error:\", mean_squared_error(y_validation, np.array(initial_prediction) ,squared=False))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"NN mean absolute error:\", mean_absolute_error(y_validation, np.array(initial_prediction)))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"NN r2 score:\", r2_score(y_validation, np.array(initial_prediction)))\n",
    "#Adjusted R Squared:\n",
    "print(\"NN adjusted r2 score:\", 1-(1-r2_score(y_validation, np.array(initial_prediction)))*((len(y_validation))-1)/((len(y_validation))-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = nn.fit(500, 1, X_train, y_train, X_validation, y_validation)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot lines\n",
    "plt.plot(range(len(losses)), [v for (t,v) in losses], label = \"validation error\")\n",
    "plt.plot(range(len(losses)), [t for (t,v) in losses], label = \"train error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_prediction = nn.predict(X_validation)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"NN mean squared error:\", mean_squared_error(y_validation, np.array(learned_prediction), squared=True))\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"NN root mean squared error:\", mean_squared_error(y_validation, np.array(learned_prediction) ,squared=False))\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(\"NN mean absolute error:\", mean_absolute_error(y_validation, np.array(learned_prediction)))\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"NN r2 score:\", r2_score(y_validation, np.array(learned_prediction)))\n",
    "#Adjusted R Squared:\n",
    "print(\"NN adjusted r2 score:\", 1-(1-r2_score(y_validation, np.array(learned_prediction)))*((len(y_validation))-1)/((len(y_validation))-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing package\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "trues = [y[0] for y in y_validation]\n",
    "trues.sort()\n",
    "initials = [p[0] for p in initial_prediction]\n",
    "initials.sort()\n",
    "predictions = [p[0] for p in learned_prediction]\n",
    "predictions.sort()\n",
    "\n",
    "# plot lines\n",
    "plt.plot(range(len(y_validation)), trues, label = \"true\")\n",
    "plt.plot(range(len(y_validation)), initials, label = \"validation predictions before fit\")\n",
    "plt.plot(range(len(y_validation)), predictions, label = \"validation predictions after fit\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = nn.predict(X_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "trues = [y[0] for y in y_test]\n",
    "trues.sort()\n",
    "test_prediction = [p[0] for p in test_predictions]\n",
    "test_prediction.sort()\n",
    "\n",
    "# plot lines\n",
    "plt.plot(range(len(y_test)), trues, label = \"true\")\n",
    "plt.plot(range(len(y_test)), test_prediction, label = \"test predictions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "X_train, y_train, X_validation, y_validation, X_test, y_test = set_data()\n",
    "import numpy as np\n",
    "X_train = np.reshape(np.array(X_train), (len(X_train), len(X_train.iloc[0])))\n",
    "y_train = np.reshape(np.array(y_train), (len(y_train), 1))\n",
    "X_validation = np.reshape(np.array(X_validation), (len(X_validation), len(X_validation.iloc[0])))\n",
    "y_validation = np.reshape(np.array(y_validation), (len(y_validation), 1))\n",
    "X_test = np.reshape(np.array(X_test), (len(X_test), len(X_test.iloc[0])))\n",
    "y_test = np.reshape(np.array(y_test), (len(y_test), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [[13, 24, 48, 24, 16, 1],\n",
    "          [13, 16, 32, 16, 1],\n",
    "          [13, 26, 1]]\n",
    "activation_funcs = [\"sigmoid\", \"tanh\", \"RelU\"]\n",
    "learning_rates = [0.1, 0.05]\n",
    "epochs = [250, 500, 750]\n",
    "batchs = [1, 4, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "grid_losses = []\n",
    "grid_weights = []\n",
    "grid_biasses = []\n",
    "grid_test_predictions = []\n",
    "grid_test_losses = []\n",
    "for i, parameters in enumerate(itertools.product(layers, activation_funcs, learning_rates, epochs, batchs)):\n",
    "    print(i,\" out of \", (len(layers)*len(activation_funcs)*len(learning_rates)*len(epochs)*len(batchs)), \" NN learning with \", parameters, sep=\"\")\n",
    "    grid_nn = NN()\n",
    "    grid_nn.create(parameters[0], parameters[1], parameters[2])\n",
    "    grid_losses.append((parameters, grid_nn.fit(parameters[3], parameters[4], X_train, y_train, X_validation, y_validation, verbose=1)))\n",
    "    grid_weights.append(grid_nn.weights)\n",
    "    grid_biasses.append(grid_nn.biasses)\n",
    "    grid_prediction_i = grid_nn.predict(X_test)\n",
    "    grid_test_losses.append(grid_nn.loss(y_test, grid_prediction_i))\n",
    "    grid_prediction_i.sort()\n",
    "    grid_test_predictions.append(grid_prediction_i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all grid search plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "trues = [y[0] for y in y_test]\n",
    "trues.sort()\n",
    "\n",
    "for i in range(len(grid_test_predictions)):\n",
    "    plt.figure()\n",
    "    plt.tight_layout()\n",
    "    sns.lineplot(x=range(len(y_test)), y=[p[0] for p in grid_test_predictions[i]], label = \"test prediction\").set(title=\"model\" + str(i) + \": \" + str(grid_losses[i][0]) + \" test loss:\" + str(grid_test_losses[i]));\n",
    "    sns.lineplot(x=range(len(y_test)), y=trues, label = \"true\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_summary = [(l, w, b, tp, tl) for l, w, b, tp, tl in zip(grid_losses, grid_weights, grid_biasses, grid_test_predictions ,grid_test_losses)]\n",
    "grid_summary.sort(key = lambda x: x[4])\n",
    "best_model_summary = grid_summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "1- Kullanılan katman sayısı:\"\"\", len(best_model_summary[0][0][0]),\"\"\"\n",
    "2- Her katmandaki nöron sayısı:\"\"\", best_model_summary[0][0][0],\"\"\"\n",
    "3- Katmanlarda kullanılan aktivasyon fonksiyonu:\"\"\", best_model_summary[0][0][1],\"\"\"\n",
    "4- Ağırlıkların ilk değerinin nasıl tanımlandığı: np.random.uniform(low=-1, high=1 size=<weight matrix shape>)\n",
    "5- Epoch Sayısı:\"\"\", best_model_summary[0][0][3],\"\"\"\n",
    "6- Normalizasyon kullanılıp, kullanılmadığı bilgisi: Kullanıldı\n",
    "7- Batch büyüklüğü:\"\"\", best_model_summary[0][0][4],\"\"\"\n",
    "8- Eğitim hatası(train loss, validation loss):\"\"\", best_model_summary[0][1][-1],\"\"\"\n",
    "9- Test hatası:\"\"\", best_model_summary[4],\"\"\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.tight_layout()\n",
    "sns.lineplot(x=range(len(y_test)), y=[p[0] for p in best_model_summary[3]], label = \"test prediction\").set(title=\"model\" + str(i) + \": \" + str(best_model_summary[0][0]) + \" test loss:\" + str(best_model_summary[4]));\n",
    "sns.lineplot(x=range(len(y_test)), y=trues, label = \"true\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
